{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"final project.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"kNiNBKIGnwi7"},"source":["import torch\n","import os\n","import cv2\n","import shutil\n","import argparse\n","import numpy as np\n","\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.utils.data import DataLoader\n","from torchvision.utils import save_image\n","from torchvision import transforms as T\n","from torchvision.transforms.functional import to_pil_image\n","from threading import Thread\n","from tqdm import tqdm\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YVGbt8n1WChf"},"source":["Set environment"]},{"cell_type":"code","metadata":{"id":"HXHCGbKLWF5p"},"source":["#{\"username\":\"lindsaymorel\",\"key\":\"9a094067d35d2f2b37e2362eda70c1ce\"}\n","\n","os.environ[\"KAGGLE_USERNAME\"] = \"lindsaymorel\"\n","os.environ[\"KAGGLE_KEY\"] = \"9a094067d35d2f2b37e2362eda70c1ce\"\n","if not os.path.exists(\"aisegmentcom-matting-human-datasets.zip\"):\n","  !kaggle datasets download -d laurentmih/aisegmentcom-matting-human-datasets/clip_img\n","  !unzip aisegmentcom-matting-human-datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVhHqKhTbKm5"},"source":[" !unzip aisegmentcom-matting-human-datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wvf3T8kxcyWx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619402607394,"user_tz":240,"elapsed":18432,"user":{"displayName":"Thomas Ramos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBOqJZrfWyeXsDqnHjH0oFRPppVNi81HDJbWvx=s64","userId":"02982434572078051406"}},"outputId":"3ab39041-ee04-4889-b3cf-963feac47934"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\", force_remount=True)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9QY2FxwcyR_c"},"source":["Parse Arguments"]},{"cell_type":"code","metadata":{"id":"m3YG-t7QyRvG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619171476566,"user_tz":-480,"elapsed":916,"user":{"displayName":"Cheng Qian","photoUrl":"","userId":"05947220755213747687"}},"outputId":"4cf90491-49c8-46be-e605-5fb206fe55a1"},"source":["#add description\n","parser =argparse.ArgumentParser(description='N/a')\n","matting_type = parser.add_mutually_exclusive_group(required=True)\n","matting_type.add_argument('--video', action='store_true')\n","matting_type.add_argument('--image', action='store_true')\n","parser.add_argument('--src', type=str, required=True)\n","parser.add_argument('--bgr', type=str, required=True)\n","parser.add_argument('--dst', type=str, required=True)\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["_StoreAction(option_strings=['--dst'], dest='dst', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, help=None, metavar=None)"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"d5tHuRBHpACu"},"source":["Create Model \n","And Load Datasets"]},{"cell_type":"code","metadata":{"id":"wXd1fKiXohLh"},"source":["if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","# model version may need to be changed\n","model = torch.hub.load('pytorch/vision:v0.6.0', model='fcn_resnet101', pretrained=True)\n","model.to(device)\n","model.eval()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EXxpSpaxTLNX"},"source":["Get the frames from video, and convert array of frames to video"]},{"cell_type":"code","metadata":{"id":"vqaN33z3TMH5"},"source":["def get_frames(path):\n","  cap= cv2.VideoCapture(path)\n","  fps=cap.get(5)\n","  i=0\n","  frames=[]\n","  while(cap.isOpened()):\n","      ret, frame = cap.read()\n","      if ret == False:\n","          break\n","          #this rotates the frame\n","      #frame=frame.transpose(1,0,2)\n","      cv2.imwrite('frame_' + str(i) + '.jpg',frame) #stored frames as jpgs\n","      frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) \n","      frames.append(frame)\n","      i+=1\n","  cap.release()\n","  cv2.destroyAllWindows()\n","  output=np.stack(frames,axis=0) #size=(N,H,W,3)\n","  #output=torch.tensor(output)\n","  return fps,output\n","\n","def frames2video(fps,output, name):\n","  output=output.numpy()\n","  out=cv2.VideoWriter(name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (output.shape[2],output.shape[1]))\n","  for i in np.arange(output.shape[0]):\n","    img=cv2.cvtColor(output[i,:,:,:], cv2.COLOR_RGB2BGR)\n","    out.write(img)\n","  out.release()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rj7xavpoZtgt"},"source":["Save session\n"]},{"cell_type":"code","metadata":{"id":"HVLCGWMiZnPs"},"source":["import dill\n","from google.colab import drive\n","\n","backup_dir = 'drive/My Drive/colab_sessions'\n","backup_file = 'notebook_env.db'\n","backup_path = backup_dir + '/' + backup_file\n","\n","def init_drive():\n","  # create directory if not exist\n","  drive.mount('drive')\n","  if not os.path.exists(backup_dir):\n","    !mkdir backup_dir\n","\n","def restart_kernel():\n","  os._exit(00)\n","\n","def save_session():\n","  init_drive()\n","  dill.dump_session(backup_path)\n","\n","def load_session():\n","  init_drive()\n","  dill.load_session(backup_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q2YRUb8Ao-Uy"},"source":["Train Model\n"]},{"cell_type":"code","metadata":{"id":"OtmIwUAgpKN_"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4K6fdGPhpK3b"},"source":["Evaluate model and composite video "]},{"cell_type":"code","metadata":{"id":"Z55xbIUPlC8l"},"source":["def predict(model,image,device):\n","  if torch.cuda.is_available():\n","    device = \"cuda\" \n","  else:\n","    device = \"cpu\"\n","  mean = [0.485, 0.456, 0.406]\n","  std = [0.229, 0.224, 0.225]\n","  process=T.Compose([T.ToTensor(), T.Normalize(mean,std)])\n","  img=process(image)\n","  batch=img.unsqueeze(0)\n","  batch.to(device)\n","  with torch.no_grad():\n","    output=model(batch)['out'][0]\n","    # output=nn.functional.softmax(output, dim=0)\n","    output=torch.argmax(output,dim=0).cpu().numpy()\n","  return output\n","\n","#REQUIRES: Foreground, Background and Segmentation all have the same shape\n","def composite(foreground, background, segmentation):\n","  print(\"sheesh\", foreground.shape, background.shape, segmentation.shape)\n","  assert foreground.shape == background.shape\n","  assert foreground.shape == segmentation.shape\n","  fgr = foreground * segmentation\n","  seg = ~segmentation\n","  \n","  bgr = background * seg\n","  print(type(fgr), fgr.shape, segmentation.shape, bgr.shape, type(segmentation), type(bgr))\n","  #s = np.asarray(segmentation)\n","  #plt.imshow(cv2.cvtColor(segmentation, cv2.COLOR_BGR2RGB))\n","  #plt.imshow(cv2.cvtColor(seg, cv2.COLOR_BGR2RGB))\n","  ##plt.imshow(cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB))\n","  #plt.imshow(cv2.cvtColor(fgr, cv2.COLOR_BGR2RGB))\n","  f = fgr > 0\n","  b = bgr > 0\n","  print(f)\n","  cv2.imwrite('/content/drive/MyDrive/EECS 442 Final Project/out/mask1.jpg', f.astype(int) * 255)\n","  cv2.imwrite('/content/drive/MyDrive/EECS 442 Final Project/out/inverted_mask1.jpg', b.astype(int) * 255)\n","\n","  cv2.imwrite('/content/drive/MyDrive/EECS 442 Final Project/out/foreground1.jpg', fgr )\n","  \n","  cv2.imwrite('/content/drive/MyDrive/EECS 442 Final Project/out/background1.jpg', bgr)\n","  return fgr + bgr\n","\n","#Image is tensor of the composited image, name is the filename, and to_drve\n","# is a bool if the file is to be saved to google drive\n","def download_image(image, name, to_drive):\n","  to_pil_image(image.cpu()).save(name)\n","  if to_drive:\n","    files.download(name)\n","\n","#Frames is the composited frame of the video as a tensor\n","def download_video(frames, fps, name, to_drive):\n","  frames2video(fps, frames, name)\n","  if to_drive:\n","    files.download(name)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fbmp4ARAN7YW"},"source":["# src is video source, bgr is background image source, dst is destination to save composite\n","def composite_video(model, srcPath, bgrPath, dst):\n","  bgr = cv2.imread(bgrPath)\n","  fps, frames = get_frames(srcPath)\n","  N, H, W, _ = frames.shape\n","  print(\"this is the shape of frames\", frames.shape)\n","  out = cv2.VideoWriter(dst, cv2.VideoWriter_fourcc(*'mp4v'), fps, (W, H))\n","  print(len(frames), fps)\n","  #todo see if any transforms are needed\n","  #background = Image.open(bgr).convert('RGB')\n","  #resize background image to be the same size as the video \n","  background = cv2.resize(bgr, (W, H), interpolation=cv2.INTER_AREA)\n","  background = cv2.cvtColor(background, cv2.COLOR_BGR2RGB)\n","  i = 0\n","  for frame in tqdm(frames):  \n","    #pred = predict(model, frames[i], device)\n","    pred = predict(model, frame, device)\n","    mask = pred == 15\n","    mask = np.repeat(mask[:, :, np.newaxis], 3, axis = 2)\n","    \n","    #comp = composite(frames[i], background, mask)\n","    comp = composite(frame, background, mask)\n","    # comp=background.copy()\n","    # comp[mask]=frame[mask]\n","    print('comp', comp.shape)\n","    #todo check that comp comes out as a tensor\n","    img=cv2.cvtColor(comp, cv2.COLOR_RGB2BGR)#[i,:,:], cv2.COLOR_RGB2BGR)\n","    print(\"comp shape\", img.shape)\n","    out.write(img)\n","    i += 1\n","    #if i > len(frames)/10:\n","    # break\n","    if i < 2:\n","      print('frame', type(frames[i]), frames[i].shape)\n","      print('comp', type(comp), comp.shape)\n","      print('background', type(background), background.shape)\n","  out.release()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4nlWi1JkPvor"},"source":["def composite_image(model,srcPath, bgrPath, dst):\n","  src = cv2.imread(srcPath)\n","  bgr = cv2.imread(bgrPath)\n","  # bgr=cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n","  print(type(src))\n","  prediction = predict(model, src, device)\n","  H, W, _ = src.shape\n","  mask = prediction == 15\n","  mask = np.repeat(mask[:, :, np.newaxis], 3, axis = 2)\n","  # background = Image.open(bgr).convert('RGB')\n","  background = cv2.resize(bgr, (W, H), interpolation=cv2.INTER_AREA)\n","  # download_image(composited, \"output.jpg\", True)\n","  composited = composite(src, background, mask)\n","  plt.imshow(cv2.cvtColor(composited, cv2.COLOR_BGR2RGB))\n","  out = cv2.imwrite(dst,composited)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fpFeQiw--Pxp"},"source":["\n","    \n","srcPath = '/content/drive/MyDrive/EECS 442 Final Project/img_set2/example.jpg'\n","#src = cv2.imread(path)\n","dst = '/content/drive/MyDrive/EECS 442 Final Project/out/composite_img3.jpg'\n","bgrPath = '/content/drive/MyDrive/EECS 442 Final Project/bgr/bgr1.jpg'\n","#bgr = cv2.imread(bgrPath)\n"," \n","#src = parser.src\n","#bgr = parser.bgr\n","#dst = parser.dst\n","\n","#Output to video format\n","#if parser.video:\n","#  composite_video(src, bgr, dst)\n","#output to image format\n","#elif parser.image:\n","composite_image(model,srcPath, bgrPath, dst)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iH4es8VsoKSp"},"source":["bgrPath = '/content/drive/MyDrive/EECS 442 Final Project/bgr/bgr3.jpg'\n","srcPath = '/content/drive/MyDrive/EECS 442 Final Project/vid/vs3_360.mp4'\n","dst = '/content/drive/MyDrive/EECS 442 Final Project/out/composite_vid3.mp4'\n","composite_video(model, srcPath, bgrPath, dst)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jmGx3RIdZkt7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619060345113,"user_tz":240,"elapsed":262,"user":{"displayName":"Thomas Ramos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBOqJZrfWyeXsDqnHjH0oFRPppVNi81HDJbWvx=s64","userId":"02982434572078051406"}},"outputId":"cb1cdd8f-3faa-4e30-c005-12e5daff22a8"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dOQfh2ufLEP6"},"source":["import moviepy.editor as mp\n","clip = mp.VideoFileClip(\"/content/drive/MyDrive/EECS 442 Final Project/vid/vs3.mp4\")\n","clip_resized = clip.resize(height=360) # make the height 360px ( According to moviePy documenation The width is then computed so that the width/height ratio is conserved.)\n","clip_resized.write_videofile(\"/content/drive/MyDrive/EECS 442 Final Project/vid/vs3_360.mp4\")"],"execution_count":null,"outputs":[]}]}